# Simple MLP

This is an implementation of a multilayer perceptron (feedworward neural network)
that can be trained with Stochastic Gradient Descent (SGD). SGD is implemented
with backpropagation.

## Example usage
To test this, download the MNIST traing data and put it
in the `mnist` directory as explained in the README.md of
that directory. I got my training data from here:
https://omnomnom.vision.rwth-aachen.de/data/mnist.tgz

After that, you can run the `mnist_softmax_regressor.py` example.

